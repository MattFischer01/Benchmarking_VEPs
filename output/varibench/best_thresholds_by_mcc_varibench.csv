"column","threshold","n","accuracy","sensitivity","specificity","precision","f1","mcc","auc"
"DEOGEN2_rankscore",0.7,9782,0.818748722142711,0.910493827160494,0.749371633752244,0.733129420760849,0.81224187228635,0.655060478219851,0.906799287664039
"REVEL_rankscore",0.6,9983,0.786537113092257,0.932246798603027,0.676511954992968,0.685147159479808,0.789821481408423,0.611791981092968,0.891069162618359
"MetaRNN_rankscore",0.8,10137,0.766893558251948,0.587375261081457,0.899622512010981,0.812259306803594,0.681750841750842,0.521767210140219,0.869886841207841
"MutScore_rankscore",0.9,10141,0.752391282910955,0.515080664016834,0.925477489768076,0.834469696969697,0.636981350296371,0.495802033069835,0.776064471724612
"AlphaMissense_rankscore",0.6,10112,0.693829113924051,0.624824684431978,0.744429208090504,0.64193083573487,0.633262260127932,0.370670346771245,0.727278773830376
"SIFT_converted_rankscore",0.7,9841,0.674423330962301,0.541002811621368,0.776601471379867,0.649690489589195,0.590386090513935,0.32768958276095,0.705733507096994
"SIFT4G_converted_rankscore",0.7,9906,0.650413890571371,0.509205313446749,0.758325912733749,0.616883116883117,0.557896080684285,0.276591770127157,0.671229704201861
"MutPred2_rankscore",0.7,10128,0.649881516587678,0.477837085170573,0.777281319814401,0.61371087928465,0.537317327766179,0.267989276724789,0.671180459686821
"CADD_raw_rankscore",0.6,10207,0.635250318408935,0.559294499883964,0.690742624618515,0.569201700519603,0.564204611963011,0.250653309208388,0.659527744850357
"PrimateAI_rankscore",0.4,9813,0.622337715275655,0.650110105211647,0.602514844568634,0.538617474153659,0.589135254988914,0.24907994557569,0.676189554623201
